[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Jason Zivkovic and I’m a Data Scientist from Melbourne, Australia, working at the Reece Group.\nSome of my passions include web scraping, package and app development, statistical modeling, sports analytics and data visualisation.\nThe bulk of my programming is done using the R programming language, with some Python thrown in."
  },
  {
    "objectID": "index.html#what-you-will-find-here",
    "href": "index.html#what-you-will-find-here",
    "title": "About Me",
    "section": "What you will find here",
    "text": "What you will find here\nThis blog will be mainly about R programming, mainly through sports, but will touch on other parts of life also, using data visualisations to tell the story. What I can promise though is it will be filled with my own selfish pleasures. I hope they align with my readers’ interests.\nSome of the sports that will be included include Australian Rules Football (AFL), Basketball, American Football, World Football (Soccer) and many more.\nThe one constant is that the data will tell the story.\nIf you don’t agree with it, please, please don’t blame the data.\nImportantly, all views expressed here are my own."
  },
  {
    "objectID": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html",
    "href": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html",
    "title": "Building a Linear Regression Model in R to Predict AFL Crowds",
    "section": "",
    "text": "There is a lot of talk about crowd behaviour and crowd issues with the modern day AFL. I personally feel the crowd issues are a case of business-as-usual; I’ve been going to AFL games on-and-off for close to three decades and the crowd behaviour is no different today as it was 20 years ago… just now everyone with a phone and Twitter account is a journo.\nAnyway, that’s my two cents worth. This analysis is not on crowd behaviour. This post is primarily concerned with building a model to predict AFL crowds during the season proper. While data was collected since the beginning of the 2000 season, the model will be bult on training data from the beginning of the 2013 season to the end of the 2018 season (Finals series excluded), with the trained model then used to predict the attendance figures of AFL games during the 2019 season up until round 18 which concluded on Sunday 21st July, 2019. The linear model will be built using R (version 3.5.3) in RStudio.\nThis post is a follow up to my introductory analysis done on bandwagon fans. It can be found here.\nThe article was inspired by the amazing work Tony Corke does, specifically his post on The Predictability of AFL Crowds.\nAll code for this project can be found on GitHub here\n\n\nThree sources of data were used in the project:\n\nAFL Tables was used as the primary source of information. This is no doubt the go to for anyone looking at doing AFL analysis\nAustralian Sports Betting was used to get betting data for matches from the 2013 season\nAustralian Bureau of Meteorology was used for climat data (rain and temperature). Part of the scraper for BOM data was taken from James Day’s fitzRoy R package. Big thanks to him. The rain and temperature data had some missing observations. Missing rain data was filled by averaging out the next taken reading over the missed data points before it. Missing temperature was filled taking the average temperature for that month."
  },
  {
    "objectID": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#features-created",
    "href": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#features-created",
    "title": "Building a Linear Regression Model in R to Predict AFL Crowds",
    "section": "Features Created",
    "text": "Features Created\nBased on some feedback from the Banwagon article and other things that I wanted to explore, some new features were created to see if they had any predictive power.\nTake the game start time as an example - having the time is great, but a regression model may find it better to categorise the time to a period of the day to reduce some of the noise. So, with that in mind, a feature (time_period) was created - if the game started by 5pm, I have classified it as an afternoon game. Games started between 5pm-7pm were classified as evening games, while games later than this were night games.\nAdditionally, the day of the week and the time_period were joined into one variable game_time to use in the model. For example, if it was a Saturday game starting at 5:30pm, it was classified as “Sat-Evening”.\nVenues that were used infrequently, or more accurately, weren’t the teams primary “home” stadium were labelled as “Other”. There were 94 games played at these venues, and this includes at stadiums like Eureka Stadium, Cazaly’s Stadium, Traeger Park, and international venues Wellington in NZ and Jiangwan Stadium in China.\nAnother feature was created to determine rivalry games - the thinking being that when two fierce rivals face off against each other, the crowds will come. To be a rivalry game, a few different options existed. The first contains the four oldest traditional rivals, all from Melbourne - Collingwood, Richmond, Essendon and Carlton (hold your Carlton opinions, I know they’ve been horrible but their fans still turn up to these rivalry games). The Derby in SA (The Showdown between Adelaide and Port Adelaide) and in WA (Western Derby between West Coast and Fremantle) are also classed as rivalry games.\nAfter hearing a key AFL staffer talk a few years back talk about betting markets being a strong predictor of attendances, I collected betting data to test this assumption. A number of features were created in addition to the raw odds and line data scraped. These included calculating the differences between the odds of the home and away team and also calculating the change in betting throughout the week.\nFeatures were also created of the average experience of both the home and away teams, as were milestone games (where players of either team are playing in their 100, 200, 250 or 300th games).\nEach team’s last game result was calculated, as was the number of wins each team had in their previous three games. Additionally, whether the team played finals the previous season was identified.\nA ladder at each round was created and ladder position, percentage and score for and against was explored as possible features for the end model.\nA feature was also created to identify games played by teams from the same state or not."
  },
  {
    "objectID": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#the-response-variable---attendance",
    "href": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#the-response-variable---attendance",
    "title": "Building a Linear Regression Model in R to Predict AFL Crowds",
    "section": "The Response Variable - Attendance",
    "text": "The Response Variable - Attendance\nLet’s look at our response variable - or the thing we’re trying to predict - attendance at AFL premiership season games.\nWe can see that the attendance figures are somewhat positively skewed. We’ll see if the few higher attendance games cause any issues later on. This variable may need to undergo some form of transformation, but more on that later.\nThe median attendance to AFL games during the regular season is just under 32,500 for the 1,340 games played since 2013."
  },
  {
    "objectID": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#some-key-predictors",
    "href": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#some-key-predictors",
    "title": "Building a Linear Regression Model in R to Predict AFL Crowds",
    "section": "Some Key Predictors",
    "text": "Some Key Predictors\n\n\nCode\ndata_for_model <- afl_premiership_season %>%\n  select(season, attendance, team1, team2, round, venue, game_time, home_median_experience, home_mean_experience, away_median_experience, away_mean_experience, home_300, home_milestone, away_300, away_milestone, count_milestones, rainfall_clean, min_temp, max_temp, last_five_days_rain, temperature_diff, HomeOddsOpen, AwayOddsOpen, HomeLineOpen, AwayLineOpen, odds_diff, rivalry_game, HomeTeamFav, team1_last_result, team2_last_result, split_round, home_team_finals_last_season, away_team_finals_last_season, season_stage, temperature, IsHomeMilestone, IsAwayMilestone, count_milestones, last_results, home_team_state, away_team_state, is_same_state, temperature_diff, home_odds_change, away_odds_change, home_line_change, away_line_change, second_home_game, home_season_points, home_score_for, home_percentage, home_ladder_pos, away_wins_last_three, away_season_points, away_score_for, away_percentage, away_ladder_pos, teams_in_eight, finals_last_season, home_wins_last_three, away_wins_last_three) %>% na.omit()\n\n\nAs was evident in the first post of this series, there was noticeable variation in attendances between the teams, and also whether teams won or lost their previous game. Finally, the first post showed that the home team’s favouritism status also seemed to have a relationship with attendance.\nCategorical Features:\nSome of the key categorical variables created that we would expect to have an impact on crowd figures are displayed below.\n\n\n\n\n\n\n\n\nSome observations:\n\nThere appears to be a relationship between the venue the game is played at and the attendance. Stadiums classified as Other include secondary home grounds and some of the ovals not used in 2019. There were 238 of the 1,340 games at these stadiums\nWhether either the home or away team played finals last year seems to have a weak relationship with attendance. It may be useful to include in our model\nWhen the game is played and attendance appear to be related. Weekday Afternoon and Friday Afternoon are high because of ANZAC day clashes between Essendon and Collingwood no doubt. Friday and Sunday night games seem to draw better crowds. Could be a useful predictor\nThere appears to be some variance in attendance figures between the different rounds during a season. Could be a useful predictor\nGames classed as “Rivalry Games” clearly draw larger attendances than non-rivalry games. Could be a useful predictor\nWhether the round was a split round (where some teams have a bye, or week off) or not doesn’t appear to be related to attendances. Might not be a useful predictor\nThere appears to be slightly higher crowds when the home team has a player playing a milestone (either a 100, 200, 250 or 300th game), but the relationship certainly doesn’t appear strong. Might not be a useful predictor\nWhen the two competing teams are from the same state, crowds appear to be higher. Could be a useful predictor\nTo be expected, games where the home team is playing the game at their second home ground (Hawthorn and North in Tasmania, Melbourne, Western Bulldogs in Darwin, etc) draw lower crowds. This is probably more to do with lower statium capacities, but may still be a useful feature\n\nNumerical Features:\nTo explore the relationship between the numerical features and attendance, Pearson’s correlation will be employed.\nUsing dplyr to select just the numerical features and then calculate the correlations on the converted matrix yields the below correlations with attendance.\n\n\nCode\nnumeric_features <- data_for_model %>% select_if(is.numeric)\n\nnumeric_features %>% \n  as.matrix() %>% \n  cor() %>% .[,\"attendance\"] %>% sort(decreasing = T)\n\n\n            attendance   home_mean_experience        home_percentage \n            1.00000000             0.26242775             0.23603629 \nhome_median_experience   home_wins_last_three   away_mean_experience \n            0.23224717             0.17740024             0.16024146 \naway_median_experience     home_season_points        away_percentage \n            0.12698259             0.10829187             0.10549367 \n  away_wins_last_three                 season           AwayLineOpen \n            0.08874187             0.06906937             0.06717077 \n      temperature_diff       count_milestones       home_line_change \n            0.04693253             0.04502199             0.03176978 \n        away_milestone               home_300         home_milestone \n            0.02947022             0.02805048             0.02612771 \n    away_season_points               away_300       home_odds_change \n            0.01964212             0.01305329             0.01234577 \n      away_odds_change         home_score_for       away_line_change \n           -0.01732249            -0.02851372            -0.03176978 \n          AwayOddsOpen         rainfall_clean         away_score_for \n           -0.05413613            -0.06190488            -0.06313146 \n          HomeLineOpen    last_five_days_rain                  round \n           -0.06717077            -0.08122135            -0.08681637 \n              max_temp               min_temp        away_ladder_pos \n           -0.09200801            -0.13811543            -0.15435393 \n          HomeOddsOpen              odds_diff            temperature \n           -0.15999933            -0.16722767            -0.20128979 \n       home_ladder_pos \n           -0.27541679 \n\n\nWhile all correlations appear to be fairly weak, the following observations can be made:\n\nThe average experience (home_mean_experience) for the home team, calculated as the average games played by each player at each game they play in, yields the highest Pearson correlation with attendance of 0.264\nThe Home team’s percentage (scores for / scores against) also have a correlation above 0.2 at 0.236\nThe number of wins the home team had in their last three games had a Pearson correlation of 0.175, while the correlation of away team’s average playing experience was 0.159\nStrangely, the home team’s ladder position had a negative correlation, with a Pearson correlation of -0.273. The away team’s ladder position also had a negative correlation of -0.150\nBetting data shows some relationships; as the HomeOddsOpen increase, the attendance tends to decrease, which is somewhat expected. The odds_diff variable (the difference between the home teams opening odds and the away teams) also has a weak negative relationship (correlation -0.168)\nWeather data shows some relationships. As expected, the maximum temperation and rainfall, both on the day and the accumulation of last five day’s rainfall has a negative relationship with attendance (-0.091, -0.081 and -0.65 respectively)"
  },
  {
    "objectID": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#things-that-were-attempted-but-not-included-in-the-model",
    "href": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#things-that-were-attempted-but-not-included-in-the-model",
    "title": "Building a Linear Regression Model in R to Predict AFL Crowds",
    "section": "Things that were attempted but not included in the model",
    "text": "Things that were attempted but not included in the model\nThe following features were thought to have some validity but when run through the model were found to have no predictive power:\n\nThe number of wins the teams had in their last three games didn’t contribute\nThe rainfall on the day had no predictive power (however the total rainfall for the five days leading up to game day did)\nMilestone games made the model perform worse"
  },
  {
    "objectID": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#whats-missing-would-be-good-to-have",
    "href": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#whats-missing-would-be-good-to-have",
    "title": "Building a Linear Regression Model in R to Predict AFL Crowds",
    "section": "What’s missing / would be good to have",
    "text": "What’s missing / would be good to have\nThere is some information that wasn’t available publicly (or I didn’t try to get) which may prove handy in any future modelling work. These include:\n\nTicket pricing for games during each season to explore the effect ticket pricing has on attendance\nWhether tickets were given away to fans for games and whether that had an impact on attendances\nAny social media posts / newspaper text analysis regarding teams / topical matters involving players playing in games, etc\n\n\n\nCode\nafl_pre_2019 <- data_for_model %>% filter(season < 2019, season >= 2013)\n\nafl_2019 <- data_for_model %>% filter(season == 2019)"
  },
  {
    "objectID": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#baseline-model",
    "href": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#baseline-model",
    "title": "Building a Linear Regression Model in R to Predict AFL Crowds",
    "section": "Baseline Model",
    "text": "Baseline Model\nBefore building a model to predict games, we want a baseline model to be able to compare to. The temptation would be there to use the average crowd for each game as a baseline, but the different venue capacities make this less than relevant. As a result, to calculate the baseline model, I will use the median attendance for each vanue (taking the average of the venues labelled “Other” as a whole), for each home-away team combo. For example, Collingwood(H) vs Essendon(A) is different to Essendon(H) vs Collingwood(A). The median is used because of the slightly skewed nature of AFL crowds.\n\n\nCode\nmedian_attendances <- afl_pre_2019 %>% \n  group_by(team1, team2, venue) %>% \n  summarise(baseline_attendance = median(attendance)) %>% ungroup()\n\nbaseline <- afl_2019 %>% select(team1, team2, venue, attendance) %>% \n  left_join(median_attendances, by = c(\"team1\", \"team2\", \"venue\")) %>% \n  filter(!is.na(baseline_attendance))\n\n\nUsing the average attendances per ground as our baseline, an RMSE of 6,816 and a mean absolute error of 4,958 is achieved. Any model that is built from here needs to be better than this."
  },
  {
    "objectID": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#transforming-response-variable",
    "href": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#transforming-response-variable",
    "title": "Building a Linear Regression Model in R to Predict AFL Crowds",
    "section": "Transforming Response Variable",
    "text": "Transforming Response Variable\nBecause of the somewhat skewed nature of our response variable, it might be wise to undergo some sort of transformation to normalise it more. A log transformation is routinely used to “unskew” data. Below is the result of log transforming the attendance variable.\nIt looks less skewed than the non-transformed variable, however (spoiler alert), the models performed worse with attendance being log transformed."
  },
  {
    "objectID": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#analysing-the-best-model",
    "href": "blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/index.html#analysing-the-best-model",
    "title": "Building a Linear Regression Model in R to Predict AFL Crowds",
    "section": "Analysing the best model",
    "text": "Analysing the best model\nThe first thing we want to do is ensure the residuals are normally distributed to satisfy the condition of normality for linear models.\nThe below histogram suggests that the errors are fairly normally distributed and centred around zero. A good sign.\n\n\nCode\nggplot(data = data.frame(fit_lm$residuals), aes(x= fit_lm.residuals)) + \n  geom_histogram() +\n  ggtitle(\"ERRORS ARE CENTRED AROUND ZERO\") +\n  scale_x_continuous(labels = comma, name = \"Model Residuals\")\n\n\n\n\n\nCode\n# ggsave(\"plots/prediction_plots/errors_hist.png\", width = 30, height = 22, units = \"cm\")\n\n\nThen we can look at the coefficients of each of the predictor variables.\nTo make inspecting these easier, the broom package was used. Using the tidy() function, the results of the summary lm output are nicely displayed in a DF.\nThe coefficients for each of the predictor variables (other than home and away teams) are displayed below.\n\n\nCode\na <- broom::tidy(fit_lm) %>% mutate(p.value = round(p.value, 5))\n\na %>% filter(!str_detect(term, \"team1\"), !str_detect(term, \"team2\")) %>%  \n  kableExtra::kable(format = \"html\", escape = F) %>%\n  kableExtra::kable_styling(\"striped\", full_width = F) \n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    61007.772536 \n    7066.622895 \n    8.6332288 \n    0.00000 \n  \n  \n    round \n    -628.797448 \n    147.199874 \n    -4.2717255 \n    0.00002 \n  \n  \n    venueCarrara \n    -9384.433835 \n    4893.988254 \n    -1.9175432 \n    0.05543 \n  \n  \n    venueDocklands \n    2025.790428 \n    1843.347042 \n    1.0989740 \n    0.27202 \n  \n  \n    venueGabba \n    -12078.813180 \n    7915.867406 \n    -1.5258989 \n    0.12732 \n  \n  \n    venueKardinia Park \n    -790.290125 \n    2477.093571 \n    -0.3190393 \n    0.74976 \n  \n  \n    venueM.C.G. \n    14604.069479 \n    1859.992224 \n    7.8516831 \n    0.00000 \n  \n  \n    venueOther \n    -11316.633901 \n    1521.161777 \n    -7.4394677 \n    0.00000 \n  \n  \n    venuePerth Stadium \n    2344.181828 \n    2119.734512 \n    1.1058846 \n    0.26902 \n  \n  \n    venueS.C.G. \n    -6985.844583 \n    2854.176355 \n    -2.4475869 \n    0.01454 \n  \n  \n    venueSydney Showground \n    -13915.330942 \n    2344.860834 \n    -5.9343952 \n    0.00000 \n  \n  \n    game_timeFri Evening \n    -14669.992661 \n    7858.171556 \n    -1.8668456 \n    0.06219 \n  \n  \n    game_timeFri Night \n    -26232.422896 \n    6419.398624 \n    -4.0864300 \n    0.00005 \n  \n  \n    game_timeSat Afternoon \n    -27517.789656 \n    6413.609760 \n    -4.2905307 \n    0.00002 \n  \n  \n    game_timeSat Evening \n    -28587.419367 \n    6425.144735 \n    -4.4493036 \n    0.00001 \n  \n  \n    game_timeSat Night \n    -27304.481145 \n    6416.346545 \n    -4.2554561 \n    0.00002 \n  \n  \n    game_timeSun Afternoon \n    -28302.693645 \n    6410.217064 \n    -4.4152473 \n    0.00001 \n  \n  \n    game_timeSun Evening \n    -30499.992506 \n    6441.388865 \n    -4.7350025 \n    0.00000 \n  \n  \n    game_timeSun Night \n    -29709.469650 \n    6969.788830 \n    -4.2626069 \n    0.00002 \n  \n  \n    game_timeWeekday Afternoon \n    -6264.436854 \n    6587.953778 \n    -0.9508927 \n    0.34187 \n  \n  \n    game_timeWeekday Evening \n    -32666.780662 \n    8987.962304 \n    -3.6345035 \n    0.00029 \n  \n  \n    game_timeWeekday Night \n    -24296.860101 \n    6490.825702 \n    -3.7432618 \n    0.00019 \n  \n  \n    I(HomeLineOpen * home_ladder_pos) \n    -3.929212 \n    0.897336 \n    -4.3787523 \n    0.00001 \n  \n  \n    rivalry_gameRivalry \n    4243.471176 \n    1103.664170 \n    3.8448935 \n    0.00013 \n  \n  \n    last_resultsboth_won \n    3278.290173 \n    599.102681 \n    5.4720005 \n    0.00000 \n  \n  \n    last_resultsonly_away_team_won \n    1354.077316 \n    536.942587 \n    2.5218289 \n    0.01181 \n  \n  \n    last_resultsonly_home_team_won \n    1381.030990 \n    594.706552 \n    2.3222058 \n    0.02040 \n  \n  \n    last_five_days_rain \n    -44.287937 \n    12.297250 \n    -3.6014505 \n    0.00033 \n  \n  \n    max_temp \n    310.452321 \n    61.108889 \n    5.0803137 \n    0.00000 \n  \n  \n    min_temp \n    -125.802409 \n    59.279520 \n    -2.1221901 \n    0.03404 \n  \n  \n    home_mean_experience \n    25.443372 \n    14.678437 \n    1.7333843 \n    0.08331 \n  \n  \n    away_mean_experience \n    45.390519 \n    14.339468 \n    3.1654256 \n    0.00159 \n  \n  \n    is_same_stateyes \n    8151.587887 \n    601.638591 \n    13.5489778 \n    0.00000 \n  \n  \n    home_score_for \n    7.394136 \n    1.692602 \n    4.3685025 \n    0.00001 \n  \n  \n    I(home_percentage * home_ladder_pos) \n    -294.795766 \n    66.179029 \n    -4.4545193 \n    0.00001 \n  \n  \n    I(home_wins_last_three * round) \n    3.645874 \n    19.738164 \n    0.1847119 \n    0.85349 \n  \n  \n    finals_last_seasonboth_played \n    1691.406848 \n    1188.541482 \n    1.4230945 \n    0.15499 \n  \n  \n    finals_last_seasonhome_only_played \n    953.657176 \n    1235.936253 \n    0.7716071 \n    0.44051 \n  \n  \n    finals_last_seasonneither_played \n    482.072356 \n    1101.460684 \n    0.4376664 \n    0.66171 \n  \n  \n    round:finals_last_seasonboth_played \n    17.403974 \n    83.889701 \n    0.2074626 \n    0.83569 \n  \n  \n    round:finals_last_seasonhome_only_played \n    -13.710531 \n    82.253765 \n    -0.1666858 \n    0.86765 \n  \n  \n    round:finals_last_seasonneither_played \n    21.455146 \n    76.032301 \n    0.2821846 \n    0.77785 \n  \n\n\n\n\n\n\n\nCode\nactual_2019crowds <- afl_2019$attendance\n\n# fit linear model \nafl_2019$predicted_attendance <- predict(fit_lm, afl_2019)\n\n# put a floor on AFL attendances - surely the AFL doesn't let the crowd fall below 6,000\nafl_2019$predicted_attendance <- ifelse(afl_2019$predicted_attendance < 6000, 6000, afl_2019$predicted_attendance)\n\n# calculate the errors\nerror <- afl_2019$predicted_attendance - afl_2019$attendance\n\n\nThen we want to annalyse how well the model fit the relationship. This can be achieved by plotting the actual vs predicted values on a scatterplot.\nI can be seen that the linear model does a fairly good job on average. There model did under-predict some of the larger drawing games, while also predicting an attendance of over 30,000 to the Hawthorn vs GWS game at the MCG in round 8, which only drew 14,636 fans.\n\n\nCode\nafl_2019 %>% \n  ggplot(aes(x=attendance, y= predicted_attendance)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept=0) +\n  ggtitle(\"MODEL UNDER-PREDICTED SOME\\nLARGE DRAWING GAMES\") +\n  scale_x_continuous(labels = comma, name = \"Actual Attendance\") +\n  scale_y_continuous(labels = comma, name = \"Predicted Attendance\") +\n  annotate(geom = \"text\", x=83000, y= 55000, label = \"Model under-predicted\\non these games\")\n\n\n\n\n\nCode\n# ggsave(\"plots/prediction_plots/actual_v_pred.png\", width = 30, height = 22, units = \"cm\")\n\n\nWe can also look at how the model performed predicting crowds at each of the venues.\n\n\nCode\nafl_2019 %>% \n  mutate(error = predicted_attendance - attendance) %>% \n  ggplot(aes(x= error)) +\n  geom_density() +\n  geom_vline(xintercept = 0, linetype = 2) +\n  facet_wrap(~ venue, scales = \"free_y\", ncol = 2) +\n  scale_x_continuous(labels = comma, name = \"Error\") +\n  ggtitle(\"Exploring Errors by Venue\") +\n  theme(axis.text.y = element_blank(), axis.title.y = element_blank())\n\n\n\n\n\nCode\n# ggsave(\"plots/prediction_plots/venue_errors.png\", width = 30, height = 22, units = \"cm\")\n\n\nThe model tended to over-predict attendances at the Gabba, Perth Stadium, Sydney Showgrounds and even the MCG, while it under-predicted at Carrara and “Other”.\n\n\nCode\nprint(paste0(\"RMSE: \", sqrt(mean(error^2, na.rm = T))))\n\n\n[1] \"RMSE: 6601.69023180541\"\n\n\nCode\nprint(paste0(\"MAE: \", mean(abs(error), na.rm = T)))\n\n\n[1] \"MAE: 4881.5087837533\""
  },
  {
    "objectID": "blog/simplifying-afl-tipping/index.html",
    "href": "blog/simplifying-afl-tipping/index.html",
    "title": "Effectively Simplifying AFL Tipping",
    "section": "",
    "text": "I feel like I always overthink footy tipping. During each round, I make myself believe I have some sort of secret sauce and conjure up visions in my head of nailing a solid roughy… and then fall flat half way through the season and give up…\nSo then I thought, surely there’s an easier way. Only problem was, I thought this up last weekend (during the last season of the round)…I wonder how many tips I would’ve gotten this year had I just picked the home team every game…\nOk, so 113 winners wouldn’t have been good enough…"
  },
  {
    "objectID": "blog/simplifying-afl-tipping/index.html#but-can-we-make-some-money",
    "href": "blog/simplifying-afl-tipping/index.html#but-can-we-make-some-money",
    "title": "Effectively Simplifying AFL Tipping",
    "section": "But can we make some money",
    "text": "But can we make some money\nOk, so now that I know that the Simple Model’* performs fairly competitively in tipping comps, I want to know if I can make some money using this method?\nTo answer this question, I’m placing a theoretic $10 on each game and seeing how many bags of cash are left at the end.\n\n\n\n\n\nHmmm… so using this method, I would’ve lost almost $40, even though it was good enough to win some tipping comps.\nOnly in 2018 would this method have worked, with a net profit of $26.82…\nThe 2017 season would’ve been the most brutal, losing just over $245 for the season.\nTo find out where everything went so wrong, we can use an animated line plot that tracks the overall profit throughout each round of the season. To get a rundown on how the animated plot was generated, see this post here.\n\n\n\n\n\n\n\n\nDuring the 2019 season, we can see that things would’ve started off really bleak - after round five, I would’ve been down over $120! Things started to pick up from there though, even being in the black at round 14. At no point during the 2015 and 2017 seasons would this model have been profitable.\nWhile keeping me competitive in footy tipping comps, this model certainly isn’t going to allow me to retire anytime soon."
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#what-data-can-i-get",
    "href": "blog/extract-data-using-worldfootballR/index.html#what-data-can-i-get",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "What data can I get?",
    "text": "What data can I get?\nThe package as at version 0.3.2 provides access to data from the following data sites:\n\nFBref.com (a whole host of data to analyse, including results, match stats, season long stats, player and team stats, etc);\nTransfermarkt.com (player market values, team transfer history, player transfer history); and,\nUnderstat.com (shot locations data for matches played in the major leagues)\n\nThese three sites are regularly used by analysts the world over, however the package is constantly evolving and may include data from additional sites in the future.\nTo my knowledge, only FBref provide the ability to export data to a file - the other two don’t, so you’d need to find some other way to get your data (painfully slow copy and paste), and that’s where worldfootballR come in."
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#prerequisites",
    "href": "blog/extract-data-using-worldfootballR/index.html#prerequisites",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe only prerequisites to this post are that you have a computer, internet connection and the desire to analyse world football data. This post is designed to take any aspiring analyst with absolutely no R coding experience to being able to extract data programmatically using the worldfootballR R library."
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#what-is-r-why-would-i-use-r",
    "href": "blog/extract-data-using-worldfootballR/index.html#what-is-r-why-would-i-use-r",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "What is R / Why would I use R?",
    "text": "What is R / Why would I use R?\nFrom R’s official site (https://www.r-project.org/about.html);\n\nR is a language and environment for statistical computing and graphics.\n\nYou can find out a lot more about R as you get more experienced with the language on the home page, but all you need to know for now is that R will be another tool in your toolkit to perform the analyses you want to do. R is an amazing programming framework that allows you to do a number of things, including data cleaning, performing statistical analysis and modeling and building fully customisable visualisations using the R programming language.\nImportantly, anything programmed means it’s repeatable. Set up your code upfront, then simply rerun it to get consistent results.\nThe world is your oyster if you have the drive to learn."
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#getting-started",
    "href": "blog/extract-data-using-worldfootballR/index.html#getting-started",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "Getting started",
    "text": "Getting started\nOk so a few things before we dive right in… this is in layman’s terms (because I myself am a layman). R is the engine, the coding language driving everything you want to do, and RStudio is the pretty graphical user interface (GUI pronounced “gooey”) that gives you additional functionality when writing code in it. I am going to propose installing both R and RStudio in this post, but if you don’t want RStudio (I advise you to go with RStudio), you could still run the code to get the data you need (I will assume here everything is done in RStudio)."
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#install-r",
    "href": "blog/extract-data-using-worldfootballR/index.html#install-r",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "Install R",
    "text": "Install R\nYou remember how I said I’m a layman? It’ll extend itself here. We’re going to install R and we’re going to use all of the default options (just keep clicking “accept” or “agree”, or “continue”, and nothing else until R has been successfully installed. You can customise the installation when you’re better versed in R, but for now, just show me the data!\nSo how to find R to install R you ask? Well, you have to type into Google some really sophisticated stuff… “download r windows” or on Mac OS… yep, you guessed it, “download r mac”.\n\nIf you can’t be bothered doing that, here are the links:\n\nFor Windows OS: https://cran.r-project.org/bin/windows/base/\n\nSelect “Download R X.X.X for Windows”, with “X.X.X” being for whatever is the most recent version of R:\n\n\nFor Mac OS: https://cran.r-project.org/bin/macosx/\n\nDepending on what version of Mac OS you’re running will determine which version to install, but generally speaking it’ll only be one of the highlighted options below:\n\nOnce you have selected either of the Windows or Mac OS versions, install as you would any software, remembering my one special rule at the start of this section… default options all the way."
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#now-for-rstudio",
    "href": "blog/extract-data-using-worldfootballR/index.html#now-for-rstudio",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "Now for RStudio",
    "text": "Now for RStudio\nOnce R has been installed, we can now go ahead and install RStudio, remember, the GUI that will allow you to code in and provide additional functionality.\nBack to our intricate Google search query… “download rstudio”, and select the first result:\n\nOr for the URL: https://www.rstudio.com/products/rstudio/download/\nOnce we’ve opened the page, we’re presented with a few pricing options, but if you’re like almost all amateur analysts (let’s be honest, even professional analysts), you’re probably only going to need the free version:"
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#were-almost-ready-to-start-coding",
    "href": "blog/extract-data-using-worldfootballR/index.html#were-almost-ready-to-start-coding",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "We’re (almost) ready to start coding!",
    "text": "We’re (almost) ready to start coding!\nWell, we’re almost ready to start coding, but first we’ll do a bit of a tour of RStudio (again, for those that have forgotten, RStudio is the software where we type and execute our R code).\n\nThe console pane is where we will be able to see our code running (or run our code in here if we’re not using the scripts), whether there are any “errors” or “warnings” and printed outputs.\nThe environment is the sections where we will see the objects that we’ve imported, or created, including data frames (think of something similar to an Excel table).\nIn the viewers pane, there are a few tabs, including being able to navigate through the file system (‘Files’ tab), any visualisations we’ve created (‘Plots’ tab), the libraries/packages we have loaded on our machines and have available to us “locally” (‘Packages’ tab), a help section, where we can get additional details on what we need to provide the functions to work and a ‘Viewer’ tab to see HTML outputs."
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#now-were-ready-to-start-coding",
    "href": "blog/extract-data-using-worldfootballR/index.html#now-were-ready-to-start-coding",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "Now we’re ready to start coding",
    "text": "Now we’re ready to start coding\nOpen up an R script (there are a number of different options to code in, but a .R script is the best place to start.\nTo open up a script, you can either press control+shift+n on a Windows machine, or command+shift+n on a Mac, or you can select a new script with the drop down in the top-left corner:\n\nChanging the layout as such:\n\nWe will start writing code in the top-left quadrant."
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#writing-code",
    "href": "blog/extract-data-using-worldfootballR/index.html#writing-code",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "Writing code",
    "text": "Writing code\nBefore I fully get in to the nuts and bolts of the code, I just want to point you to some resources.\nA website has been created for the worldfootballR package that will give you a full run down of all the functions available and the arguments you need to pass to those functions to get the data you want. The website can be found here.\nNow we can really start writing code.\nAs we’ve already established, packages contain functions (code to do something). To use these packages they need to be installed (downloaded onto our laptops) first.\nPackages will typically (but not limited to) either be housed on CRAN (a central repository for all packages) or on GitHub. worldfootballR is housed on GitHub, so the instructions for installing this package are slightly different to the packages housed on CRAN. Before we can install worldfootballR, we need to install the devtools package to enable the installation of worldfootballR. We will also install another package, xlsx that will enable us to save files in ‘.xlsx’ format.\nWe type (just copy and paste) these lines of code into our R script the first time we use R OR when we update our version of R OR the package updates its functionality:\n# to allow us to install packages from github:\ninstall.packages(\"devtools\")\n\n# a meta package that contains packages for data cleaning, analysis and visualisation\ninstall.packages(\"tidyverse\")\n\n# to save xlsx files\ninstall.packages(\"xlsx\")\nYou’ll know that they have successfully installed if there are no error messages and you can see the you see the “>” symbol and a blinking cursor in the console section.\n\nThen we will install worldfootballR (it’s what we’re all here for) and importantly load the library - it’s not enough to just install the package, we need to load it to make the functions easily available in our current session.\nNote: We only need to install the package the first time we install R OR if we update our version of R, OR if there is a update to a newer version of the package.\ndevtools::install_github(\"JaseZiv/worldfootballR\", ref = \"main\")\nNote 2: While we only need to install it very infrequently, we need to load the package every time we want to use the package.\nlibrary(worldfootballR)"
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#the-world-is-your-oyster",
    "href": "blog/extract-data-using-worldfootballR/index.html#the-world-is-your-oyster",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "The world is your oyster",
    "text": "The world is your oyster\nThere are a number of functions in worldfootballR to enable you to get all the data you need.\nTo find these functions, you can visit the reference page on the package website here. The reference page will give you a brief summary of what the function does - for more detailed info, click on the function and it’ll take you to the function page.\nAdditionally, there are instructions and example code on the website for each of the sites data is collected from:\n\nClick here for FBref data\nClick here for Transfermarkt data\nClick here for Understat data"
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#lets-get-some-data",
    "href": "blog/extract-data-using-worldfootballR/index.html#lets-get-some-data",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "Let’s get some data",
    "text": "Let’s get some data\nNow the fun part - getting data!\nBefore we get started, in the code below, when we see a “#” at the beginning of text, that is a comment, not executable code, and it’s how R programmers write notes to themselves, or others who use the code to give an overview of what is being done.\nAt the start of every script, it is best practise to include all the packages you’re going to be using/needing in your analysis. Installing packages only needs to be done the first time you start using R, when installing new versions or R, or when newer versions of the package become available and you want the updates that have been created.\nThe code we would run is below. A few things to remember:\n\nTo ‘run’ code we’ve typed out, there are two main ways;\n\n\nHighlight the code you want to run and press control+enter or command+enter on MacOS\nHave the cursor on the line you want to run and press control+enter or command+enter on MacOS\n\n\nA “#” in a script signifies comments, and I will use them in the code to tell you what we’re doing\n\n\n\nCode\n# remember, we need to ensure we've installed the packages we need, but we need to do this very infrequently ( to run the 'install' lines below, simply delete the '#' before the code:\n# install.packages(\"devtools\")\n# install.packages(\"xlsx\")\n# install.packages(\"tidyverse\")\n# devtools::install_github(\"JaseZiv/worldfootballR\", ref = \"main\")\n\n\n# once it's been installed, then we need to load the functions (code to let you get the data you want) in the library (this needs to be done every time you want to run your script)\nlibrary(worldfootballR)\nlibrary(tidyverse)\nlibrary(xlsx)\n\n\n\nI want shooting data!\nLet’s say we want to get the shooting data from FBref for teams in the 2020-2021 EPL season, to be able to analyse how many goals a team scored, or how they performed against expectation (xG), or from what average distance they shot from:\n\n\n\nSource: FBref https://fbref.com/en/comps/9/Premier-League-Stats\n\n\nThen we run the specific code to extract shooting data from FBref.com:\n\n\nCode\n# now let's get our season team shooting data from FBref:\nprem_2021_shooting <- get_season_team_stats(country = \"ENG\", gender = \"M\", season_end_year = \"2021\", tier = \"1st\", stat_type = \"shooting\")\n\n\n\n\n\nAnd this is what the data looks like in R now:\n\n\n\nI just want the data and nothing else\nIf all you want is a csv or Excel output (to take to some other software/framework), then the below lines of code will address that need:\n\n\nCode\n# to save the file as a csv:\nwrite.csv(x= prem_2021_shooting, file = \"EPL_shooting_2021_season.csv\", row.names = FALSE)\n\n# or we can save an .xlsx file:\nwrite.xlsx(x= prem_2021_shooting, file = \"EPL_shooting_2021_season.xlsx\", row.names = FALSE)\n\n\nAnd this is how our output looks, and this output can be fed in to any visualisation software we choose to use:\n\n\n\nBut R has more to offer right?\nR is a whole lot more than a tool for extracting and saving data.\nWe can create fully customisable visualisations with code to analyse our data. This flexibility means your creative juices would only be constrained by your desire to practise and learn and practise and learn R.\nBelow, we have plotted a team’s performance against expected goals for the season. Teams that finished above the line scored more goals that xG would have had them scored, while teams below the dashed line scored less.\n\n\nCode\n# get our data ready for plotting\nprem_2021_shooting %>% \n  # filter out only the team shooting data, not their opponents also\n  filter(Team_or_Opponent == \"team\") %>% \n  # create a new column that removes penalties from the team's goal total\n  mutate(non_P_Gls = Gls_Standard - PK_Standard) %>% \n  # start plotting:\n  ggplot(aes(x= npxG_Expected, y= non_P_Gls)) +\n  # add a line through the plot with slope =  1 and the yintercept = 0\n  geom_abline(slope = 1, intercept = 0, colour = \"red\", linetype=2) +\n  # we want to make it a scatter plot\n  geom_point(size=6, colour=\"midnightblue\", fill=\"midnightblue\", alpha = 0.4, shape=21) +\n  # lets also add team name labels\n  ggrepel::geom_text_repel(aes(label = Squad), colour = \"midnightblue\", size=5) + \n  # limit the x and y-axis to start at 10 and finish at 100\n  scale_x_continuous(limits = c(10,100), name = \"Non-Pen xG\") +\n  scale_y_continuous(limits = c(10,100), \"Non-Pen Goals\") +\n  # create a title and subtitle - all plots should have this\n  ggtitle(\"DID TEAMS SCORE AS EXPECTED?\",\n          subtitle = \"Teams above the dashed line exceeded their xG for the\\nseason, while teams below didn't\") +\n  # apply a pre-programmed general theme:\n  theme_minimal() +\n  # but then we can customise our plot even more - first we make the background black:\n  # change the title and subtitle format\n  theme(plot.title = element_text(size=28, face=\"bold\"), plot.subtitle = element_text(size=22, colour=\"grey30\"),\n        # and change where the plot is aligned - in this case it's left-aligned\n        plot.title.position = \"plot\", plot.caption.position = \"plot\",\n        # change the size of aixs titles and text\n        axis.title = element_text(size=16), axis.text = element_text(size = 14))\n\n\n\n\n\nWe could also calculate how closely correlated xG is to expected goals using the cor() function in R, which we see printed out to be 0.8805275 - highly correlated indicating teams score pretty close to expected:\n\n\nCode\n# first we will create a new data set of only the team's performance, and not how their opponents played against them\nprem_team <- prem_2021_shooting %>% \n  filter(Team_or_Opponent == \"team\")\n\n# now we calculate the Pearson correlation\ncor(prem_team$Gls_Standard, prem_team$xG_Expected)\n\n\n[1] 0.8931299\n\n\n\n\nMore data please\nFBref.com is a great site that has given analysts unprecedented access to their data, including the ability to extract to csv’s manually. This is great when there is only one extract you need, but when there are multiple extracts, then this becomes infinitely more time consuming! Functions have been created for these, and the official worldfootballR site will get you going with them.\nOthers, like Transfermarkt.com and Understat.com don’t give you the ability to export to files. worldfootballR is here to help there too!\n\n\nCode\n# let's get the transfer balances for the 2020/21 Bundesliga season\nteam_balances <- tm_team_transfer_balances(country_name = \"Germany\", start_year = 2020)\n\n\n\n\nCode\n# to save the file as a csv:\nwrite.csv(x= team_balances, file = \"bundesliga_transfer_balances2020_2021.csv\", row.names = FALSE)\n\n# or we can save an .xlsx file:\nwrite.xlsx(x= team_balances, file = \"bundesliga_transfer_balances2020_2021.xlsx\", row.names = FALSE)\n\n\n\n\nCode\n# let's start creating our plot:\nteam_balances %>% \n  # the new two lines use 'mutate()' from the dplyr package to create or change new columns\n  # here, we create a net_transfer_income column that subtracts the expenditure form income\n  mutate(net_transfer_income = income_euros - expenditure_euros) %>% \n  # and we can also come up with a flag for if the income is above or below 0\n  mutate(green = net_transfer_income > 0) %>% \n  # ggplot is how we visualise our data\n  ggplot(aes(x=net_transfer_income, y= squad, fill = green)) +\n  geom_col() +\n  # manually select colours to use:\n  scale_fill_manual(values = c(\"darkred\", \"darkgreen\"), name = \"Made\\nMoney?\") +\n  # change the data labels on the x-axis to be formatted to currency:\n  scale_x_continuous(labels = scales::dollar, name = \"Net Transfer Income\") +\n  # add a caption that sources the data:\n  labs(caption = \"Source: transfermarkt.com\") +\n  # we also want a title and subtitle:\n  ggtitle(\"BUNDESLIGA SPENDING IN THE 2020/21 SEASON\",\n          subtitle = \"Bayern Munich and Borussia Dortmund are big net spenders this season,\\nwhile Bayer 04 Leverkusen have made the most money on the transfer market\") +\n  # apply a pre-programmed general theme:\n  theme_minimal() +\n  # but then we can customise our plot even more - first we make the background black:\n  theme(plot.background = element_rect(fill = \"black\"),\n        # play around with the x and y gridlines:\n        panel.grid.major.x = element_line(colour = \"grey20\", linetype = 2), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_blank(),\n        # change the title and subtitle format\n        plot.title = element_text(size=28, colour = \"white\", face=\"bold\"), plot.subtitle = element_text(size=22, colour = \"white\"),\n        # and change where the plot is aligned - in this case it's left-aligned\n        plot.title.position = \"plot\", plot.caption.position = \"plot\",\n        # change the colour and size of aixs titles and text, remove the y-axis title\n        axis.title.x = element_text(colour = \"white\", size=16), axis.title.y = element_blank(), axis.text = element_text(colour = \"white\", size = 14),\n        # remove the legend\n        legend.position = \"none\",\n        #format the plot caption:\n        plot.caption = element_text(size = 12, colour = \"white\"))"
  },
  {
    "objectID": "blog/extract-data-using-worldfootballR/index.html#resources",
    "href": "blog/extract-data-using-worldfootballR/index.html#resources",
    "title": "ANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE",
    "section": "Resources",
    "text": "Resources\n\nworldfootballR\nFBref\nUnderstat\nTransfermarkt\ntidyverse - a collection of very valuable R packages for manipulating, analysing and visualising data"
  },
  {
    "objectID": "blog/big3-grand-slam-difficulty/index.html#introduction",
    "href": "blog/big3-grand-slam-difficulty/index.html#introduction",
    "title": "Rating the Difficulty of the Big 3’s Grand Slam Wins",
    "section": "Introduction",
    "text": "Introduction\nAs the current (2023) Australian Open came to a close with Novak Djokovic winning a record equaling 22nd Grand Slam, the expected discourse on who is the best player of all time is back on the agenda. One discussion thread centers around the ease of Djokovic run to the 2023 Australian Open title, and that maybe this title isn’t worth as many as others (listen to the the Wharton Moneyball podcast talk about it at around the 84 minute mark here).\nThis post won’t be about which player is the best of all time - that’s already been done ad nauseam - but what it will aim to do is better understand the difficulty of each of the big three’s grand slams wins and whether we can infer anything from that.\nI’ve tried to answer this question using some numbers. The main metric being used in this analysis is Elo ratings. I’m not going to go into the guts of the Elo rating system, other than to say that it performs exceptionally well in explaining skill levels of players in competitive head to head matches and predicting the outcome of these games using the difference in each players’ rating. Specifically for this analysis, when we refer to the Elo rating, we’re referring to the player’s relevant pre-tournament surface-specific Elo rating.\nThere are a number of ways that this could be achieved. Presented in this post are only a few ways to do that. I’d be really keen for some engagement on this post on whether you agree/disagree with the methods employed and where the holes are in these methods used.\nTo freshen our memories, below are the slam wins each of the big three have achieved over the different slams:\n\n\n\n\n\n\nThe Data\nAll completed Grand Slams since the beginning of 2001 up until the end of the 2023 Australia Open have been analysed for this post. That’s 88 grand slams, with the notable exception of the COVID-19 forced cancellation of the 2020 Wimbledon event.\n\n\nAcknowledgements\nBefore we dive in to the analysis, I need to offer thanks to a few people. First, Elo data was obtained from the brilliant site Ultimate Tennis Statistics. Elo ratings specific to the surface being played on has been used. Their calculations on coming up with an elo rating is explained here.\nNext, I’d love to thank Jeff Sackmann for his brilliant Tennis Data repository, especially for the ATP results data found here.\nBig thanks to Andrew Whelan (who has a brilliant site Wheelo Ratings providing statistics, records and Elo ratings for a number of sports) for being a sounding board, as have some great colleagues at work.\nFinally, there have been many inspirations for this post, including the thought-provoking discussions had on the Wharton Moneyball podcast anytime tennis is brought up."
  },
  {
    "objectID": "blog/big3-grand-slam-difficulty/index.html#analysis",
    "href": "blog/big3-grand-slam-difficulty/index.html#analysis",
    "title": "Rating the Difficulty of the Big 3’s Grand Slam Wins",
    "section": "Analysis",
    "text": "Analysis\nTo get started and to understand the Elo ratings of players, we can take a look at the distribution of surface-specific Elo ratings for players playing in grand slams.\nThe peaks for all tournaments at 1,500 needs explaining - when a player enters a tournament, they may not necessarily have played enough on the surface, so as a default these players are given a rating of 1,500 for this analysis (Ultimate Tennis Statistics starts a player off on a rating of 1,500 also).\nThe distribution of Elo ratings appear to be distributed fairly similarly across the four grand slams. There are more players who have had an Elo of 1,500 for Wimbledon, but other than that, the differences are fairly negligible.\nImportant to note with the distribution of each tournaments, having a pre-tournament Elo rating greater than 2,250 (two standard deviations from the mean) is pretty rare, and even rarer still for a player to have an Elo rating higher than 2,250 that isn’t one of the big 3.\n\n\n\n\n\n\n\n\n\n\n\nElo Criticism\nOne (probably valid) criticism of Elo ratings is that the average Elo rating can change over time and across eras so may not be comparable over time (e.g. an Elo rating of 2,000 in 2005 may not be equivalent to 2,000 in 2023). This needs to be considered when assessing the results in the below sections.\nTo get an idea of how these ratings have changed over time, the average Elo rating of competitors at each grand slam is plotted below. Important to note that the y-axis scale (Elo ratings) doesn’t start at zero, so the fluctuations are accentuated.\n\n\n\n\n\nLooking at individual grand slams and the distribution of Elo ratings of the opponents of the big three in their grand slam wins, we can see that they typically have faced very similar rated players. Federer’s median opponent Elo rating in his tournament wins has been sightly the highest of the three (2,014, Djokovic 2,012 and Nadal 2,002), however his average (arithmetic mean) is slightly lower at 1,992 than that of Djokovic (2,004). Nadal has both the lowest median and average opponent Elo in his grand slam wins. This is because Federer hasn’t played as many high rated outliers as Djokovic has (and to a lesser extent Nadal).\n\n\n\n\n\n\n\n\nStandardising Elo\nA potential flaw in using the Elo ratings of each tournament is that it doesn’t normalise for the distribution of talent (Elo ratings) over time. Some year’s tournaments may see more higher rated players competing, while other tournaments may contain weaker fields. When comparing across different time periods, it might be better to attempt to standardise each opponents’ strength rating.\nOne way to do this is to calculate the standard score (z-score) for each player’s rating, which essentially ranks the players rating within the confines of the distribution of the space selected - in this case each tournament over the entire 22 and a bit years. I won’t go into the full details of the z-score, however a simple way to think about it is a z-score of zero indicates the opponent’s Elo rating is average. The higher it goes above zero, the better the Elo rating is historically (conversely the further below zero it goes the worse the Elo rating).\nFor example, Stafanos Tsitsipas’s pre 2023 Aus Open Elo rating was 2,176, which gives a z-score of 1.62 for the 2023 Aus Open, however the z-score for all Aus Opens over the period analysed is 1.57. Given that his historical z-score is slightly lower suggests that the 2023 Aus Open may have been generally weaker overall than other years.\nWe can then sum these historical z-scores up for each tournament and give a comparable strength score for each grand slam win. Put simply, the higher this number is, the tougher the opponents faced for their respective winning tournaments.\nTo compare each of the big three over their respective winning grand slams, we can take an average tournament z-score of opponents ratings to see which of the three had tougher tournament opponents.\nThis method shows that Djokovic has beaten the historically toughest opponents on average during his 22 grand slams. The average total z-score of his opponents has been 5.7, followed by Federer’s 5.3. Nadal lags behind the three at 4.9, indicating that his 22 grand slams may have been slightly easier (relative of course) on average than the other big three.\n\n\n\n\n\n\n\n  \n  \n    \n      Name\n      Average Tournament Strength\n    \n  \n  \n    NOVAK DJOKOVIC\n5.722\n    RAFAEL NADAL\n4.905\n    ROGER FEDERER\n5.342\n  \n  \n  \n\n\n\n\n\n\n\nExpected Win Probability\nInstead of using the raw Elo ratings and standardising them, we can also infer each of the big three’s win probability using their’s and their opponent’s Elo ratings.\nTo do this, we first need to calculate the win probability of each match given each player’s Elo rating. For example, the win probability for Djokovic (Elo=2,452) in his fourth round match against Alex De Minaur (Elo=2,076) was 89.7%. From here, we take the product of each win probability and come up with an overall win probability. For example, Djokovic’s 2023 Aus Open win probabilities for each of his matches were 97.5%, 99.6%, 93.1%, 89.7%, 86.7%, 91.4%, 83.0% from his first round match to the final, respectively. Multiplying each of these win probabilities give us a total win probability of 53.3%. The closer we get to 100%, the more we expect that player to have won the tournament. This can also be a simple place holder for the strength of each tournament win - closer to 100%, the easier their run was.\nUnder this method, we can see that Djokovic’s 2008 Australian Open win (his first grand slam) was the least expected win of all 64 slams these three have won with an expected win probability of 3.24%. Additionally, three of the five least expected slam wins have been by Djokovic. Federer’s first grand slam (Wimbledon 2003) was really close to being the least expected (3.55%).\n\n\n\n\n\n\n\n  \n    \n      LEAST PROBABLE TOURNAMENT WINS\n    \n    \n  \n  \n    \n      Year\n      Slam\n      Name\n      Tournament Win Probability\n    \n  \n  \n    2008\nAustralian Open\nNOVAK DJOKOVIC\n3.24%\n    2003\nWimbledon\nROGER FEDERER\n3.55%\n    2011\nWimbledon\nNOVAK DJOKOVIC\n5.51%\n    2008\nWimbledon\nRAFAEL NADAL\n6.92%\n    2011\nAustralian Open\nNOVAK DJOKOVIC\n10.25%\n  \n  \n  \n\n\n\n\n\nWithout looking deeply at Djokovic’s 2008 Australian Open run and only looking at his final opponent (France’s Jo Wilfried Tsonga), it would be easy to believe that with an expected win probability in that final of 86.9% that this was an easy title win. Unpacking this a little deeper shows that he needed to get through Australia’s own Lleyton Hewitt in the round of 16 with a win probability of 67.5% (better than a coin flip), David Ferrer in the quarters at 54.2% (basically a coin flip) and Federer at 14.2% (a heavy underdog) in the semi-final before he had a chance to play in the final.\n\n\n\n\n\n\n\n  \n    \n      DJOKOVIC'S 2008 AUSTRALIAN OPEN OPPONENTS\n    \n    \n  \n  \n    \n      Year\n      Slam\n      Round\n      Opponent\n      Match Win Probability\n    \n  \n  \n    2008\nAustralian Open\nR128\nBENJAMIN BECKER\n89.05%\n    2008\nAustralian Open\nR64\nSIMONE BOLELLI\n91.86%\n    2008\nAustralian Open\nR32\nSAM QUERREY\n87.43%\n    2008\nAustralian Open\nR16\nLLEYTON HEWITT\n67.50%\n    2008\nAustralian Open\nQF\nDAVID FERRER\n54.16%\n    2008\nAustralian Open\nSF\nROGER FEDERER\n14.23%\n    2008\nAustralian Open\nF\nJO-WILFRIED TSONGA\n86.92%\n  \n  \n  \n\n\n\n\n\nConversely, Djokovic’s 2022 Wimbledon title was the most expected of all the big three’s grand slams, with a tournament win probability of 85.7%. Djokovic’s 2021 Wimbledon title was the fifth most expected title (72.1%).\n\n\n\n\n\n\n\n  \n    \n      MOST PROBABLE TOURNAMENT WINS\n    \n    \n  \n  \n    \n      Year\n      Slam\n      Name\n      Tournament Win Probability\n    \n  \n  \n    2022\nWimbledon\nNOVAK DJOKOVIC\n85.66%\n    2006\nAustralian Open\nROGER FEDERER\n81.58%\n    2010\nRoland Garros\nRAFAEL NADAL\n80.30%\n    2007\nAustralian Open\nROGER FEDERER\n73.33%\n    2021\nWimbledon\nNOVAK DJOKOVIC\n72.08%\n  \n  \n  \n\n\n\n\n\nNot hard to believe when we look at his run in that tournament - his lowest individual match win probability came in the final (92.5%), where he defeated Australia’s very erratic Nick Kyrgios. Other than that, it looked a very soft title.\n\n\n\n\n\n\n\n  \n    \n      DJOKOVIC'S 2022 WIMBLEDON OPPONENTS\n    \n    \n  \n  \n    \n      Year\n      Slam\n      Round\n      Opponent\n      Match Win Probability\n    \n  \n  \n    2022\nWimbledon\nR128\nSOON WOO KWON\n98.74%\n    2022\nWimbledon\nR64\nTHANASI KOKKINAKIS\n99.72%\n    2022\nWimbledon\nR32\nMIOMIR KECMANOVIC\n97.82%\n    2022\nWimbledon\nR16\nTIM VAN RIJTHOVEN\n99.72%\n    2022\nWimbledon\nQF\nJANNIK SINNER\n99.72%\n    2022\nWimbledon\nSF\nCAMERON NORRIE\n96.70%\n    2022\nWimbledon\nF\nNICK KYRGIOS\n92.48%\n  \n  \n  \n\n\n\n\n\nWhile it was neat looking at individual slams in the above section, we can aggregate all the tournament win probabilities to better understand how well each have done in getting to the amount of slams they’ve won given how many they were expected to have won.\nOf the three, Federer’s average tournament win probability over his 20 slam wins was the highest, averaging 44.3% win probability. If we were to sum up all of the individual tournament win probabilities and arrive at an expected number of wins, we can see that Federer was in fact expected to have won more slams than Djokovic if the slams they won were repeated over and over again.\nSimilarly, Rafa’s average win probability was a touch lower than Federer’s, but given he has won two more slams, his expected wins are even higher than Djokovic’s.\n\n\n\n\n\n\n\n  \n    \n      PERFORMANCE AGAINST EXPECTATION\n    \n    \n      Comparing the big 3s expected wins and their actual wins\n    \n  \n  \n    \n      Name\n      Number Slams\n      Average Tourney Win Probability1\n      Expected Wins2\n      Wins above Expected3\n    \n  \n  \n    NOVAK DJOKOVIC\n22\n36.50%\n8.03\n13.97\n    RAFAEL NADAL\n22\n42.61%\n9.38\n12.62\n    ROGER FEDERER\n20\n44.28%\n8.86\n11.14\n  \n  \n  \n    \n      1 The average of all winning slam's win probabilities\n    \n    \n      2 Calculated as the sum of all tournament win probabilities (only for the tournaments they won)\n    \n    \n      3 The difference between the slams win total minus the Expected Wins\n    \n  \n\n\n\n\n\nIt’s also interesting to note how each of the big three’s expected tournament win probabilities have changed over their career.\nPre-2020, Nadal has consistently been expected to win at a higher rate than the other two - most likely a result of his Roland Garros dominance. That win probability has been dropping on average in recent years as he’s started winning on his “least” preferred surfaces. Conversely, Djokovic’s win probability has been increasing over time."
  },
  {
    "objectID": "blog/big3-grand-slam-difficulty/index.html#finishing-up",
    "href": "blog/big3-grand-slam-difficulty/index.html#finishing-up",
    "title": "Rating the Difficulty of the Big 3’s Grand Slam Wins",
    "section": "Finishing Up",
    "text": "Finishing Up\nSo what does this tell us?\nIn this post I’ve tried to use Elo to quantify the difficulty rating of each superstar’s phenomenal achievements in getting to the number of slams they have individually. Again, as stated at the very beginning of this post, this is in no way an attempt to rank the big three - far from it.\nWe did however see that the metrics used in this post would suggest that it’s not true that Djokovic’s has had an easy ride during his 22 grand slams wins. They have been quite the opposite on both fronts; the opponents he’s faced over his 22 wins seemingly rated more highly than that of Nadal and Federer, while also having outperformed his expected win probability by a greater margin than that of the other big three.\nWhile it’s a shame the majestic Roger Federer ever needed to retire, I’m really excited to continue to enjoy the once in a lifetime experience of seeing multiple transcendent stars continue to dominate their sport concurrently.\nWould love feedback on the post and have holes poked in it wherever they need poking!\nThanks!"
  },
  {
    "objectID": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html",
    "href": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html",
    "title": "Using Data To Determine Which AFL Fans Jump On The Bandwagon",
    "section": "",
    "text": "This post is the first in a two-part series on AFL crowds. This analysis will be a thorough look at AFL crowd numbers over the last 20 years, and will aim to discover which team’s fans are the biggest bandwagon jumpers. Bandwagon Jumpers are those fans that are the loudest in the office or in your friendship group when things are going well, and the quietest when their team is losing. The analysis will use two questions to answer this:\n\nWhich team’s home attendance is affected more because of the previous week’s performance\n\nHere I will introduce three new metrics to measure this effect\n\nDoes a team’s betting odds affect home attendance\n\nPeople all have their theories on which team’s fans are the worst offending bandwagoners; this analysis will try to answer it once and for all! Also, sefishly, I’m sick of people telling me that fans of my team - the Hawthorn Hawks (a team that has experienced tremendous success over the last 10 years) are the worst offenders.\n\n\nData was collected from a few different online sources. The game and attendance data was sourced using the rvest package from the amazing AFL Tables, while the betting data was sourced from the Australian Sports Betting website.\nAll the code and data for this project is available on GitHub. The Tidyverse suite of packages features heavily throughout.\nThe data is current to round 16 of the 2019 season.\n\n\n\n\n\n\n\nOver the last 20 years, the median (the 50% mark and a better measure than the average on skewed data) crowd to AFL games was 33,600. The data is positively skewed with a tail that extends out to 100,000.\n\n\n\n\n\nAs expected for the AFL, the finals series at the end of the season draws larger crowds than the premiership (regular season for non-AFL fans) season. The median attendance for the premiership season is just over 33,000, while for finals it is over 58,000. The finals series are the flagship games for the AFL - they would hope these games would draw larger crowds.\n\n\n\n\n\nWe can’t glean a lot from finals since they’re well patronised irrespective of who’s playing, so the rest of the analysis will focus on the premiership season.\nThe last 20 seasons have seen some peaks and throughs though, with median attendances ranging from just under 29,000 in 2012 to just over 36,800 in 2008. The low attendances in 2012-13 were no doubt fuelled by the introduction of the newest expansion teams - Gold Coast and Greater Western Sydney (GWS) - in regions that are traditionally not AFL strongholds. The last five years however have seen a rebound of sorts, as these clubs slowly build up fanbases (and in the case of GWS start to have some success)."
  },
  {
    "objectID": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#losing-impact-on-attendance-lioa",
    "href": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#losing-impact-on-attendance-lioa",
    "title": "Using Data To Determine Which AFL Fans Jump On The Bandwagon",
    "section": "Losing Impact on Attendance (LIoA)",
    "text": "Losing Impact on Attendance (LIoA)\nHmmmm… well that certainly doesn’t help my case. Hawthorn’s Losing Impact on Attendance (LIoA) rating is -11.9% - a league worst, meaning that Hawthorn fan attendance at games after a loss is 13% lower than their season median home attendance. Another Victorian club, St Kilda, aren’t far behind, with a LIoA of -10.7%. GWS and Collingwood follow closely, with an LIoA of -7.9 and -7.6% respectively.\nAt the other end of the spectrum, Geelong supporters appear to be unaffected by their team losing their prior game, with a 0.6% difference between their median home attendance. The Gold Coast Suns and South Australian club Adelaide are also fairly unaffected, with LIoA of -1.8% and -1.9% respectively."
  },
  {
    "objectID": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#winning-impact-on-attendance-wioa",
    "href": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#winning-impact-on-attendance-wioa",
    "title": "Using Data To Determine Which AFL Fans Jump On The Bandwagon",
    "section": "Winning Impact on Attendance (WIoA)",
    "text": "Winning Impact on Attendance (WIoA)\nConversely, we can apply the same methodology to calculate a team’s Winning Impact on Attendance (WIoA) to determine which fans respond more positively after a win. The Melbourne Football Club’s fans appear to respond the most positively after their team win, with a WIoA of 13.6%. St Kilda are second on this list too, with a WIoA of 11.3% and North Melbourn are at 11.2%. Thankfully, my Hawks don’t top this measure, however with a WIoA of 10.3%, they’re not far behind.\nAgain, Geelong fans have the best WIoA at less than half a percent, while Gold Coast and Essendon are also both under 2%."
  },
  {
    "objectID": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#overall-performance-impact",
    "href": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#overall-performance-impact",
    "title": "Using Data To Determine Which AFL Fans Jump On The Bandwagon",
    "section": "Overall Performance Impact",
    "text": "Overall Performance Impact\nCalculating the difference between the two measures give an overall indicator of fan senstitvity to their team’s last performance. I hate to say it, but Hawthorn fans top this list with an OPI of 22.2%, just edging out St Kilda on 22.0%. Melbourne and GWS are at around the 18% mark, while North, Richmond, Collingwood and Carlton in the 16-17% range.\nThis pains me to say, but Geelong fans are the most consistent set of fans with an OPI of 1.1% Gold Coast, Adelaide and Essendon supporters are also fairly consistent in their attendance.\n\n\n\n\n \n  \n    Team \n    Median Home Attendance \n    Attendance After Loss \n    Attendance After Win \n    LIoA \n    WIoA \n    OPI \n  \n \n\n  \n    Adelaide \n    41897.5 \n    41095.0 \n    42415.0 \n    -0.0192 \n    0.0124 \n    0.0316 \n  \n  \n    Brisbane Lions \n    25403.0 \n    23943.0 \n    26872.0 \n    -0.0575 \n    0.0578 \n    0.1153 \n  \n  \n    Carlton \n    35147.5 \n    33115.5 \n    38743.5 \n    -0.0578 \n    0.1023 \n    0.1601 \n  \n  \n    Collingwood \n    48261.0 \n    44607.0 \n    52592.0 \n    -0.0757 \n    0.0897 \n    0.1654 \n  \n  \n    Essendon \n    43947.0 \n    42617.0 \n    44736.0 \n    -0.0303 \n    0.018 \n    0.0483 \n  \n  \n    Fremantle \n    34553.0 \n    33539.5 \n    36026.0 \n    -0.0293 \n    0.0426 \n    0.0719 \n  \n  \n    Geelong \n    24857.0 \n    24659.0 \n    25078.0 \n    -0.008 \n    0.0089 \n    0.0169 \n  \n  \n    Gold Coast \n    13247.5 \n    13080.0 \n    13528.0 \n    -0.0126 \n    0.0212 \n    0.0338 \n  \n  \n    Greater Western Sydney \n    10069.5 \n    9395.5 \n    11071.5 \n    -0.0669 \n    0.0995 \n    0.1664 \n  \n  \n    Hawthorn \n    31925.0 \n    28152.5 \n    35202.0 \n    -0.1182 \n    0.1026 \n    0.2208 \n  \n  \n    Melbourne \n    28707.0 \n    27266.0 \n    32621.0 \n    -0.0502 \n    0.1363 \n    0.1865 \n  \n  \n    North Melbourne \n    24062.0 \n    22754.0 \n    26763.0 \n    -0.0544 \n    0.1123 \n    0.1667 \n  \n  \n    Port Adelaide \n    28206.0 \n    26652.0 \n    30197.0 \n    -0.0551 \n    0.0706 \n    0.1257 \n  \n  \n    Richmond \n    39713.5 \n    36821.0 \n    43240.0 \n    -0.0728 \n    0.0888 \n    0.1616 \n  \n  \n    St Kilda \n    30497.0 \n    26974.0 \n    33944.0 \n    -0.1155 \n    0.113 \n    0.2285 \n  \n  \n    Sydney \n    29264.0 \n    27715.5 \n    30863.5 \n    -0.0529 \n    0.0547 \n    0.1076 \n  \n  \n    West Coast \n    38029.0 \n    36710.0 \n    39436.0 \n    -0.0347 \n    0.037 \n    0.0717 \n  \n  \n    Western Bulldogs \n    27818.5 \n    26301.0 \n    28769.0 \n    -0.0546 \n    0.0342 \n    0.0888"
  },
  {
    "objectID": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#underdog-impact",
    "href": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#underdog-impact",
    "title": "Using Data To Determine Which AFL Fans Jump On The Bandwagon",
    "section": "Underdog Impact",
    "text": "Underdog Impact\nWhen the home team is the underdog, what is the impact to their home attendance?\nCarlton fans appear to be the most affected by their team’s chance of victory - when they’re the underdog, Carlton’s home attendance is 8.5% lower than their overall median home attendance. Strangely, Melbourne’s home attendance actually increases when they’re the underdog, with the attendance 13.7% higher.\nHawthorn feature near the top of this list too… not a good sign."
  },
  {
    "objectID": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#favourite-impact",
    "href": "blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/index.html#favourite-impact",
    "title": "Using Data To Determine Which AFL Fans Jump On The Bandwagon",
    "section": "Favourite Impact",
    "text": "Favourite Impact\nWow… Ok this doesn’t look great. Hawthorn’s median attendance when favourite is over 45% higher than their overall home attendance since 2013. Geelong’s, just over 40%, is also abnormally high.\n\n\n\n\n\nWell, that didn’t work out how I expected. Rather than busting the myth that Hawthorn fans are the biggest bandwagon jumpers in the league, I’ve actually added to the argument.\nStay tuned for part two of this series, where I will attempt to build a predictive model that predicts AFL attendances.\nFeel free to leave any feedback / corrections / etc in the comments or get in touch."
  },
  {
    "objectID": "blog/deploy-r-shiny-heroku-docker-github-actions/index.html",
    "href": "blog/deploy-r-shiny-heroku-docker-github-actions/index.html",
    "title": "Deploying an R Shiny App on Heroku via GitHub Actions and Dockerfile to Access Private GitHub Repositories",
    "section": "",
    "text": "I’ve deployed a few R Shiny apps now on Heroku that have been containerised using Docker and run from a Github Action and found the process fairly seamless (well as seamless as Dev Ops for a hack goes). The approach worked wonderfully for installing public packages from CRAN and reading in data from public GitHub repositories.\nThis time though in my Heroku deployed R Shiny app, I needed a way to load in data from a private GitHub Releases repository AND install an R library that I’d written - which is also in a private GitHub repository.\nThis post is going to build on a super helpful post I’ve come across that has helped me on a few R Shiny app deployments on Heroku. The post Deploying Shiny Apps to Heroku with Docker and GitHub Actions by Peter Solymos can be found here.\nThe below instructions will also assume you have Docker installed on your machine, have set up a GitHub account, Heroku account, and on the heroku account have set up the dynos (app containers) you need."
  },
  {
    "objectID": "blog/deploy-r-shiny-heroku-docker-github-actions/index.html#deploying-a-r-shiny-app-on-heroku-using-docker-and-github-actions",
    "href": "blog/deploy-r-shiny-heroku-docker-github-actions/index.html#deploying-a-r-shiny-app-on-heroku-using-docker-and-github-actions",
    "title": "Deploying an R Shiny App on Heroku via GitHub Actions and Dockerfile to Access Private GitHub Repositories",
    "section": "Deploying a R Shiny app on Heroku using Docker and GitHub Actions",
    "text": "Deploying a R Shiny app on Heroku using Docker and GitHub Actions\nPeter’s super helpful post has served me well and almost did everything I needed in this specific situation. To recap everything in those instructions (and a few other steps to absolutely complete the end-to-end):\n\nBuild your app, create a sub-directory called app/ and save the app in project/app/. Any other files you also need for your app (data, functions, environments, etc, it is easier if they are also saved in app/, unless you don’t need those files for the app to run, as explained in this post)\nFor reproducibility, use renv or some other package manager to ensure a consistent environment. Call renv::init() to capture dependencies in the renv.lock file\nIn your project root, create a Dockerfile and paste the below contents in there, replacing the value in LABEL maintainer to your own details:\n\n# change `r-base:latest` to another valid version if you want to pin a specific R version\nFROM rocker/r-base:latest\n\n# change maintainer here\nLABEL maintainer=\"Your Name <your.email.address.com>\"\n\n# add system dependencies for packages as needed\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    sudo \\\n    libcurl4-gnutls-dev \\\n    libcairo2-dev \\\n    libxt-dev \\\n    libssl-dev \\\n    libssh2-1-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# we need remotes and renv\nRUN install2.r -e remotes renv\n\n# create non root user\nRUN addgroup --system app \\\n    && adduser --system --ingroup app app\n\n# switch over to the app user home\nWORKDIR /home/app\n\nCOPY ./renv.lock .\nRUN Rscript -e \"options(renv.consent = TRUE);renv::restore(lockfile = '/home/app/renv.lock', repos = c(CRAN = 'https://cloud.r-project.org'), library = '/usr/local/lib/R/site-library', prompt = FALSE)\"\nRUN rm -f renv.lock\n\n# copy everything inside the app folder\nCOPY app .\n\n# permissions\nRUN chown app:app -R /home/app\n\n# change user\nUSER app\n\n# EXPOSE can be used for local testing, not supported in Heroku's container runtime\nEXPOSE 3838\n\n# web process/code should get the $PORT environment variable\nENV PORT=3838\n\n# command we want to run\nCMD [\"R\", \"-e\", \"shiny::runApp('/home/app', host = '0.0.0.0', port=as.numeric(Sys.getenv('PORT')))\"]\nOPTIONAL: To test in a local docker container:\n\nBuild the container using sudo docker build -t image_name . replacing image_name with anything you want to call the image. Don’t forget to add the . at the end of the docker build command\nThen test the container using docker run -p 6543:3838 image_name and then visit 127.0.0.1:4000 to see your app in all its glory\n\n\nLog in to Heroku. In the dashboard, click on ‘New’ then select ‘Create new App’.\nGive a name (e.g. shiny-example, if available, this will create the app at https://shiny-example.herokuapp.com/) to the app and create the app\nIn your Heroku dashboard, go to your personal settings\nFind your API key, click on reveal and copy it, you’ll need it later\nGo to the Settings tab of the GitHub repository, scroll down to Secrets and add your HEROKU_EMAIL and HEROKU_API_KEY as repository secrets\nIn the project directory locally, create the directory .github/workflows/ and then create a yml file called deploy.yml (or you can call this anything really)\nPut the below in the deploy.yml file you created at step 6, remembering to change the heroku_app_name variable to the name of your app. Note, the building and pushing of the Docker image to the Heroku container registry is based on the akhileshns/heroku-deploy GitHub action:\n\nname: Build Shiny Docker Image and Deploy to Heroku\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  app1:\n    name: Build and deploy Shiny app\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n      - name: Build and push Docker to Heroku\n        uses: akhileshns/heroku-deploy@v3.12.12\n        with:\n          heroku_app_name: shiny-example\n          appdir: \".\"\n          heroku_api_key: ${{ secrets.HEROKU_API_KEY }}\n          heroku_email: ${{ secrets.HEROKU_EMAIL }}\n          usedocker: true\nYour project should now look something like this:\n+-- .Rproj.user\n+-- .github/workflows\n|   +-- deploy.yml\n+-- project.Rproj\n+-- .gitignore\n+-- Dockerfile\n+-- app\n|   +-- app.R\n|   +-- data-df.rds\n|   +-- globals.R\n+-- renv\n+-- renv.lock\n\nTo trigger a build, commit to your remote repository on GitHub, go to the Actions tab and you should see it starting to build. Hope for a green tick and your app should then be displayed at https://shiny-example.herokuapp.com/"
  },
  {
    "objectID": "blog/deploy-r-shiny-heroku-docker-github-actions/index.html#solving-this-problem-accessing-private-github-repositories-in-a-dockerfile-run-from-a-github-action",
    "href": "blog/deploy-r-shiny-heroku-docker-github-actions/index.html#solving-this-problem-accessing-private-github-repositories-in-a-dockerfile-run-from-a-github-action",
    "title": "Deploying an R Shiny App on Heroku via GitHub Actions and Dockerfile to Access Private GitHub Repositories",
    "section": "Solving this problem: Accessing private GitHub repositories in a Dockerfile run from a GitHub Action",
    "text": "Solving this problem: Accessing private GitHub repositories in a Dockerfile run from a GitHub Action\nSo as mentioned, the above section has served me well many times, but once I needed to access content from private repositories in a shiny app deployed on Heroku with a Dockerfile run on GitHub Actions, I came unstuck.\nHere I will label the steps I took to get around this.\nIf you haven’t created a GitHub Personal Access Token (PAT) and given it permissions to access private repositories, do so now. Do this in the Settings menu of your GitHub account. Call it something other than GITHUB_PAT - for this example, we’ll name it PRIVATE_REPO_PAT.\nStore the name of the PAT and the value somewhere secure as you’ll need this next.\nGo and add that secret(s) in the app settings on Heroku in the section called ‘Config Vars’ here.\nThen we need to update our deploy.yml file by adding the below to the end of deploy.yml:\n\n          docker_build_args: |\n            GITHUB_PAT\n        env:\n          GITHUB_PAT: ${{ secrets.PRIVATE_REPO_PAT }}\nThe full deploy.yml should now look like the below:\nname: Build Shiny Docker Image and Deploy to Heroku\n\non:\n  push:\n    branches:\n      - main\n      - master\n\njobs:\n  deploy:\n    name: Build and deploy Shiny app\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n      - name: Build and push Docker to Heroku\n        uses: akhileshns/heroku-deploy@v3.12.12\n        with:\n          # this is the Heroku app name you already set up in dashboard\n          heroku_app_name: nbl-r-shiny\n          # app directory needs to be set relative to root of repo\n          appdir: \".\"\n          # secrets need to be added to the GitHub repo settings\n          heroku_api_key: ${{ secrets.HEROKU_API_KEY }}\n          heroku_email: ${{ secrets.HEROKU_EMAIL }}\n          # don't change this\n          usedocker: true\n          docker_build_args: |\n            GITHUB_PAT\n        env:\n          GITHUB_PAT: ${{ secrets.PRIVATE_REPO_PAT }}\nNow we also need to update the Dockerfile by adding the below after the first FROM statement:\nARG GITHUB_PAT=default\nENV GITHUB_PAT=$GITHUB_PAT\nThe full Dockerfile should look like the below (again, remembering to change to your own maintainer details):\nFROM rocker/shiny:4.1.0\n\n# set env var\nARG GITHUB_PAT=default\nENV GITHUB_PAT=$GITHUB_PAT\n\n\n# change maintainer here\nLABEL maintainer=\"Your Name <your.email.address.com>\"\n\n# add system dependencies for packages as needed\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    sudo \\\n    libcurl4-gnutls-dev \\\n    libcairo2-dev \\\n    libxt-dev \\\n    libssl-dev \\\n    libssh2-1-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# we need remotes and renv\nRUN install2.r -e remotes renv\n\n# create non root user\nRUN addgroup --system app \\\n    && adduser --system --ingroup app app\n\n# switch over to the app user home\nWORKDIR /home/app\n\nCOPY ./renv.lock .\nRUN Rscript -e \"options(renv.consent = TRUE);renv::restore(lockfile = '/home/app/renv.lock', repos = c(CRAN = 'https://cloud.r-project.org'), library = '/usr/local/lib/R/site-library', prompt = FALSE)\"\nRUN rm -f renv.lock\n\n# copy everything inside the app folder\nCOPY app .\n\n# permissions\nRUN chown app:app -R /home/app\n\n# change user\nUSER app\n\n# EXPOSE can be used for local testing, not supported in Heroku's container runtime\nEXPOSE 3838\n\n# web process/code should get the $PORT environment variable\nENV PORT=3838\n\n# command we want to run\nCMD [\"R\", \"-e\", \"shiny::runApp('/home/app', host = '0.0.0.0', port=as.numeric(Sys.getenv('PORT')))\"]\nTo test that this has worked in a local Docker container, simply run docker run --env GITHUB_PAT=ghp_1234 -p 6543:3838 image_name, replacing ghp_1234 with your actual value for PRIVATE_REPO_PAT and image_name with the actual Docker image name.\nFinally, commit changes to your remote repository on GitHub, wait for your green build, go to the app URL and you should be up and running.\n Hope you have found this helpful!"
  },
  {
    "objectID": "blog/deploy-r-shiny-heroku-docker-github-actions/index.html#acknowledgements",
    "href": "blog/deploy-r-shiny-heroku-docker-github-actions/index.html#acknowledgements",
    "title": "Deploying an R Shiny App on Heroku via GitHub Actions and Dockerfile to Access Private GitHub Repositories",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nSpecial thanks to Peter Solymos again for the post listed in the intro.\nAdditionally, massive thanks to Steve Condylios and Tan Ho for their massive help getting to this solution."
  },
  {
    "objectID": "blog/first-open-source-contribution/index.html",
    "href": "blog/first-open-source-contribution/index.html",
    "title": "The Agony and the Ecstasy of my first open source contribution",
    "section": "",
    "text": "For the last year or so, I’ve had this desire to contribute to an open source R package, but like a lot of people, I found the thought of tackling the task frightening.\nWhile I work in a really dynamic and close team every day, and in the world of remote repositories (Git), I’ve had really limited exposure to collaborative working in these remote repositories… We tend more to work on projects largely on our own, so the concepts of pull requests (PRs), merging, forking… well it was all a bit daunting.\nThe following is a glimpse at the journey, and will be explained in this post:\nWhat this post won’t be is an exhaustive step-by-step guide of every touch point, rather a medium-high level summary.\nWith George’s words in mind, I thought time to push myself to jump in.\nScrolling through Twitter (as one does when nursing a newborn), I came across a tweet about a package I’ve used in a few analyses on Don’t Blame the Data that said that the package was now live on CRAN (a great achievement).\nThis naturally led me to the repository on github, at which point I noticed there were open “Issues”, and one of these being for a function to create a ladder for any round."
  },
  {
    "objectID": "blog/first-open-source-contribution/index.html#the-fitzroy-package",
    "href": "blog/first-open-source-contribution/index.html#the-fitzroy-package",
    "title": "The Agony and the Ecstasy of my first open source contribution",
    "section": "The fitzRoy Package",
    "text": "The fitzRoy Package\nThe fitzRoy package, created by James Day, is a package designed to help R users extract and analyse Australian Football League (AFL) data for both the men’s and women’s competitions:\n\nThe goal of fitzRoy is to provide a set of functions that allows for users to easily get access to AFL data from sources such as afltables.com and footywire.com. There are also tools for processing and cleaning that data.\n\nWhile I certainly haven’t done any extensive analysis on this point, I would guess that a large proportion of all AFL data analytics projects are completed with the help of this package."
  },
  {
    "objectID": "blog/first-open-source-contribution/index.html#jumping-right-in",
    "href": "blog/first-open-source-contribution/index.html#jumping-right-in",
    "title": "The Agony and the Ecstasy of my first open source contribution",
    "section": "Jumping right in",
    "text": "Jumping right in\nSo rather than think about how good it would be to contribute, why not just get in touch with James and offer to address the open issue…\n\nJames was super easy to deal with, and boy was he helpful (and patient with this bumbling fool).\nThen came the time to write the function. Well sort of write the function. Fortunately, I had already written this function for a linear regression model I built for predicting the attendance of AFL home and away games here. The function was aptly named return_ladder()… I’m a Data Scientist, not a poet.\nThe function was modified somewhat though to take advantage of the get_match_results() function in the package to return the starting data frame for return_ladder When writing the function, I wanted to address the requirement that the ladder be returned for any round, and for it to be returned for even earlier than the 2011 season, which another API already offered.\nWith that in mind, the function written takes in three arguments, all of which have the option of being blank, as well as specified:\n\nmatch_results_df - A data frame extracted using get_match_results(),\nseason_round - The round of the season the user wants the ladder for,\nseason - The season the ladder is required for.\n\nIf these are all left blank, the function will return the ladder for every round of every season since the 1897 season.\nHaving the function written was one thing, it also required roxygen notes, that are returned to the user in the help docs of the function. Hadley’s R Packages book does a good job explaining these."
  },
  {
    "objectID": "blog/first-open-source-contribution/index.html#im-ready-to-be-a-contributor",
    "href": "blog/first-open-source-contribution/index.html#im-ready-to-be-a-contributor",
    "title": "The Agony and the Ecstasy of my first open source contribution",
    "section": "I’m ready to be a contributor",
    "text": "I’m ready to be a contributor\nI’ve written the function, the help docs, and have checked the package using devtools::check() to make sure I haven’t made any mistakes that would cause the package to fail it’s build… Nothing looks alarming (well there are some warnings about No visible binding for global variable or something but I’m sure there’s nothing to worry about), but all looked good to me.\nMy local changes were committed and a PR was made, I’m ready to be a contributor, and then bam! Failed codecov!! What is that?! An email to James and I’m told it’s because there were no tests written. Ok cool, I’ll write some tests… WHAT ARE TESTS?! HOW DO I WRITE THESE TESTS?! I found this post to be really helpful, as well as Hadley’s tests in the R Packages book.\nOnce these tests were written, I commit my changes, I’m ready to be a contributor, and then bam! Changes have been made to the master that I haven’t got in my PR… ok so I need to merge the master in my PR - easy (for some maybe, I have no idea). A bit of googling, seems pretty easy, but after typing git merge origin/master, I get this editor pop up in terminal:\n\nMy initial thoughts? What the is this?!\nBit of googling, ok, it’s a VIM editor. Easy. Write a commit message and then all should be good… WAIT?! How do I get out of this screen?! Bit more googling and after typing :WQ, we’re ready to rock."
  },
  {
    "objectID": "blog/first-open-source-contribution/index.html#what.-am.-i.-doing",
    "href": "blog/first-open-source-contribution/index.html#what.-am.-i.-doing",
    "title": "The Agony and the Ecstasy of my first open source contribution",
    "section": "What. Am. I. Doing?!!",
    "text": "What. Am. I. Doing?!!\nOk so things were looking good. I’d committed my changes, all checks passed, happy days.\nYou know that line I had earlier about well there are some warnings about ‘No visible binding for global variable’ or something but I’m sure there’s nothing to worry about?? Well that was nagging away at me, because as James had advised, these would cause issues when trying to include the update on CRAN. So I fixed those, and also updated the Men’s vignette. It’s at this point that I’m a bit hazy on what I did, but all I know is is that I must have spun myself into a Git web…\nThe Master of my forked repo was two commits behind my branch Ladder, which was five commit’s ahead of Origin/Master. What. Am. I. Doing?! Trial and error, error and trial. After much heartache (I can’t stress enough how much heartache), eventually, I got myself all sorted, created another PR and… SUCCESS!!!\nFinally I can say I have successfully made my first contribution to an open source project. I hope that users of this package find the function useful and as with everything, can find improvements to make it even better."
  },
  {
    "objectID": "pages/worldfootballR.html",
    "href": "pages/worldfootballR.html",
    "title": "worldfootballR",
    "section": "",
    "text": "The worldfootballR package is designed to allow users to extract world football (soccer) data from very popular and thorough websites fbref.com, transfermarkt.com and understat.com.\nThe vignette for the package offers a full description of the package and the functions available, and can be found here."
  },
  {
    "objectID": "pages/chessR.html",
    "href": "pages/chessR.html",
    "title": "chessR",
    "section": "",
    "text": "The chessR package is designed to allow users to extract their game data and other data from popular online chess platforms, including chess.com and Lichess. The websites offer a very convenient set of APIs to be able to access date through.\nThe vignette for the package offers a full description of the package and the functions available, and can be found here."
  },
  {
    "objectID": "pages/bettRtab.html",
    "href": "pages/bettRtab.html",
    "title": "bettRtab",
    "section": "",
    "text": "This package is designed to allow users to obtain clean and tidy TAB betting markets for both racing and sports. It gives users the ability to access data more efficiently.\nThe vignette for the package offers a full description of the package and the functions available, and can be found here."
  },
  {
    "objectID": "pages/nblR.html",
    "href": "pages/nblR.html",
    "title": "nblR",
    "section": "",
    "text": "The nblR package allows users to obtain basketball statistics for the Australian basketball league (NBL). Stats include play-by-play, shooting locations, results and box scores for teams and players.\nThe vignette for the package offers a full description of the package and the functions available, and can be found here."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Deploying an R Shiny App on Heroku via GitHub Actions and Dockerfile to Access Private GitHub Repositories\n\n\n\n\n\n\n\nR\n\n\nShiny\n\n\nHeroku\n\n\nGitHub Actions\n\n\nDocker\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nJason Zivkovic\n\n\n\n\n\n\n  \n\n\n\n\nRating the Difficulty of the Big 3’s Grand Slam Wins\n\n\n\n\n\n\n\nR\n\n\nelo\n\n\ntennis analytics\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nJason Zivkovic\n\n\n\n\n\n\n  \n\n\n\n\nANALYSE FOOTBALL (SOCCER) DATA IN R WITH ZERO R EXPERIENCE\n\n\n\n\n\n\n\nR\n\n\nworldfootballR\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2021\n\n\nJason Zivkovic\n\n\n\n\n\n\n  \n\n\n\n\nThe Agony and the Ecstasy of my first open source contribution\n\n\n\n\n\n\n\nR\n\n\nfitzRoy\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2019\n\n\nJason Zivkovic\n\n\n\n\n\n\n  \n\n\n\n\nEffectively Simplifying AFL Tipping\n\n\n\n\n\n\n\nR\n\n\nafl\n\n\ngganimate\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2019\n\n\nJason Zivkovic\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Linear Regression Model in R to Predict AFL Crowds\n\n\n\n\n\n\n\nR\n\n\nafl\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2019\n\n\nJason Zivkovic\n\n\n\n\n\n\n  \n\n\n\n\nUsing Data To Determine Which AFL Fans Jump On The Bandwagon\n\n\n\n\n\n\n\nR\n\n\nafl\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2019\n\n\nJason Zivkovic\n\n\n\n\n\n\nNo matching items"
  }
]